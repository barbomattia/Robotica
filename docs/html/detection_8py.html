<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ROBOTICS PROJECT: /home/mattia/trento_lab_home/ros_ws/src/Robotica/vision_planner/src/vision_planner/detection.py File Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ROBOTICS PROJECT
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_175e3d4f5af34234ca7c7608ac2bfd84.html">vision_planner</a></li><li class="navelem"><a class="el" href="dir_d343303dcd6dc49afad74d2cbbd34260.html">src</a></li><li class="navelem"><a class="el" href="dir_553619d386e9829ce28190bd8c638940.html">vision_planner</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle">
<div class="title">detection.py File Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Performs block detection based on pre-trained ONNX model and OpenCV's Deep Neural Network submodule.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classvision__planner_1_1detection_1_1Detection.html">vision_planner.detection.Detection</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Main structure encapsulating all useful information about a single deteceted entity.  <a href="classvision__planner_1_1detection_1_1Detection.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classvision__planner_1_1detection_1_1Image.html">vision_planner.detection.Image</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Structure to get input image for neural network.  <a href="classvision__planner_1_1detection_1_1Image.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ac996652bfe7aa7af9b5d6a679aaaaaa7"><td class="memItemLeft" align="right" valign="top">cv.dnn.Net&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="detection_8py.html#ac996652bfe7aa7af9b5d6a679aaaaaa7">vision_planner.detection.loadONNX</a> ()</td></tr>
<tr class="memdesc:ac996652bfe7aa7af9b5d6a679aaaaaa7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load a neural network from an ONNX file.  <a href="detection_8py.html#ac996652bfe7aa7af9b5d6a679aaaaaa7">More...</a><br /></td></tr>
<tr class="separator:ac996652bfe7aa7af9b5d6a679aaaaaa7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a52e9c13985ac4d984a08e144c64eefe5"><td class="memItemLeft" align="right" valign="top">Image&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="detection_8py.html#a52e9c13985ac4d984a08e144c64eefe5">vision_planner.detection.padAndResize</a> (cv.Mat frame)</td></tr>
<tr class="memdesc:a52e9c13985ac4d984a08e144c64eefe5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Resize and pad an input image to a square shape for compatibility with YOLOv8 network input.  <a href="detection_8py.html#a52e9c13985ac4d984a08e144c64eefe5">More...</a><br /></td></tr>
<tr class="separator:a52e9c13985ac4d984a08e144c64eefe5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7361123c9a0b78a6c80f0ef3d3e26637"><td class="memItemLeft" align="right" valign="top">list&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="detection_8py.html#a7361123c9a0b78a6c80f0ef3d3e26637">vision_planner.detection.inference</a> (cv.Mat frame, cv.dnn.Net net)</td></tr>
<tr class="memdesc:a7361123c9a0b78a6c80f0ef3d3e26637"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs inference on an image based on a pre-trained YOLOv8 neural network converted to ONNX.  <a href="detection_8py.html#a7361123c9a0b78a6c80f0ef3d3e26637">More...</a><br /></td></tr>
<tr class="separator:a7361123c9a0b78a6c80f0ef3d3e26637"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d7cdbaeafdf5527964d9d636776db96"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="detection_8py.html#a7d7cdbaeafdf5527964d9d636776db96">vision_planner.detection.showBBox</a> (cv.Mat frame, list detections)</td></tr>
<tr class="memdesc:a7d7cdbaeafdf5527964d9d636776db96"><td class="mdescLeft">&#160;</td><td class="mdescRight">Show on screen the image on which inference has been performed with the detections.  <a href="detection_8py.html#a7d7cdbaeafdf5527964d9d636776db96">More...</a><br /></td></tr>
<tr class="separator:a7d7cdbaeafdf5527964d9d636776db96"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:adb884d107e5d0c82923b54bbbe115062"><td class="memItemLeft" align="right" valign="top"><a id="adb884d107e5d0c82923b54bbbe115062"></a>
string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="detection_8py.html#adb884d107e5d0c82923b54bbbe115062">vision_planner.detection.ONNX</a> = 'yolov8.onnx'</td></tr>
<tr class="memdesc:adb884d107e5d0c82923b54bbbe115062"><td class="mdescLeft">&#160;</td><td class="mdescRight">Path to ONNX model. <br /></td></tr>
<tr class="separator:adb884d107e5d0c82923b54bbbe115062"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d2ee39ccb3d15c42ae547f1c7be3bbf"><td class="memItemLeft" align="right" valign="top"><a id="a7d2ee39ccb3d15c42ae547f1c7be3bbf"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="detection_8py.html#a7d2ee39ccb3d15c42ae547f1c7be3bbf">vision_planner.detection.SQUARE</a> = 640</td></tr>
<tr class="memdesc:a7d2ee39ccb3d15c42ae547f1c7be3bbf"><td class="mdescLeft">&#160;</td><td class="mdescRight">Size of network input. <br /></td></tr>
<tr class="separator:a7d2ee39ccb3d15c42ae547f1c7be3bbf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6293857e7ff4e1064bc4663cda5fd85f"><td class="memItemLeft" align="right" valign="top"><a id="a6293857e7ff4e1064bc4663cda5fd85f"></a>
tuple&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="detection_8py.html#a6293857e7ff4e1064bc4663cda5fd85f">vision_planner.detection.SIZE</a> = (SQUARE, SQUARE)</td></tr>
<tr class="memdesc:a6293857e7ff4e1064bc4663cda5fd85f"><td class="mdescLeft">&#160;</td><td class="mdescRight">XY size of network input. <br /></td></tr>
<tr class="separator:a6293857e7ff4e1064bc4663cda5fd85f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae00adc6e075cf608c0a9091631d3a471"><td class="memItemLeft" align="right" valign="top"><a id="ae00adc6e075cf608c0a9091631d3a471"></a>
float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="detection_8py.html#ae00adc6e075cf608c0a9091631d3a471">vision_planner.detection.SCORETHRESH</a> = 0.25</td></tr>
<tr class="memdesc:ae00adc6e075cf608c0a9091631d3a471"><td class="mdescLeft">&#160;</td><td class="mdescRight">Threshold for confidence scores. <br /></td></tr>
<tr class="separator:ae00adc6e075cf608c0a9091631d3a471"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9f4a477441c3ec6383be994ed9d2507f"><td class="memItemLeft" align="right" valign="top"><a id="a9f4a477441c3ec6383be994ed9d2507f"></a>
float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="detection_8py.html#a9f4a477441c3ec6383be994ed9d2507f">vision_planner.detection.NMSTHRESH</a> = 0.50</td></tr>
<tr class="memdesc:a9f4a477441c3ec6383be994ed9d2507f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Non-Maximum Suppression threshold. <br /></td></tr>
<tr class="separator:a9f4a477441c3ec6383be994ed9d2507f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8fac2cd10d176dabed674d0884371b0a"><td class="memItemLeft" align="right" valign="top">list&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="detection_8py.html#a8fac2cd10d176dabed674d0884371b0a">vision_planner.detection.classNames</a></td></tr>
<tr class="memdesc:a8fac2cd10d176dabed674d0884371b0a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Classes of possibly detected objects.  <a href="detection_8py.html#a8fac2cd10d176dabed674d0884371b0a">More...</a><br /></td></tr>
<tr class="separator:a8fac2cd10d176dabed674d0884371b0a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac3a7faea5d1611ccee6dbb12c67ec2e3"><td class="memItemLeft" align="right" valign="top"><a id="ac3a7faea5d1611ccee6dbb12c67ec2e3"></a>
cv.dnn.Net&#160;</td><td class="memItemRight" valign="bottom"><b>vision_planner.detection.net</b> = loadONNX()</td></tr>
<tr class="separator:ac3a7faea5d1611ccee6dbb12c67ec2e3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a52cc62f23918323d3dc17c1d81cc7d16"><td class="memItemLeft" align="right" valign="top"><a id="a52cc62f23918323d3dc17c1d81cc7d16"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>vision_planner.detection.frame</b> = cv.imread(f'./photos/photo1.jpg')</td></tr>
<tr class="separator:a52cc62f23918323d3dc17c1d81cc7d16"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0f1d5e5c73ac7870f1a435cf20c1912f"><td class="memItemLeft" align="right" valign="top"><a id="a0f1d5e5c73ac7870f1a435cf20c1912f"></a>
list&#160;</td><td class="memItemRight" valign="bottom"><b>vision_planner.detection.detections</b> = inference(frame, net)</td></tr>
<tr class="separator:a0f1d5e5c73ac7870f1a435cf20c1912f"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Performs block detection based on pre-trained ONNX model and OpenCV's Deep Neural Network submodule. </p>
<h1><a class="anchor" id="libraries_main"></a>
Libraries/Modules</h1>
<ul>
<li>OpenCV (<a href="https://pypi.org/project/opencv-python/">https://pypi.org/project/opencv-python/</a>)<ul>
<li>Pre-process the image to perform inference on.</li>
<li>Input the image to the network.</li>
<li>Get outputs of neural network.</li>
</ul>
</li>
<li>Numpy (<a href="https://numpy.org/">https://numpy.org/</a>)<ul>
<li>Manage network output to extrapolate useful data. </li>
</ul>
</li>
</ul>
</div><h2 class="groupheader">Function Documentation</h2>
<a id="a7361123c9a0b78a6c80f0ef3d3e26637"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7361123c9a0b78a6c80f0ef3d3e26637">&#9670;&nbsp;</a></span>inference()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> list vision_planner.detection.inference </td>
          <td>(</td>
          <td class="paramtype">cv.Mat&#160;</td>
          <td class="paramname"><em>frame</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">cv.dnn.Net&#160;</td>
          <td class="paramname"><em>net</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Performs inference on an image based on a pre-trained YOLOv8 neural network converted to ONNX. </p>
<p>This function takes as input an image, pre-processes it to the required standards set by the neural network, and forward passes the blob to the DNN. Afterwards, it receives the outputs in YOLOv8 format [batchSize, 84, 8400] and processes them to extract meaningful data about the bounding boxes, class types, and confidence scores. Non-Maximum Suppression is then immediately applied to filter out possible overlapping or invalid bounding boxes, and a list of all successful detections is passed back.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">frame</td><td>Input cv2 image to perform inference on. </td></tr>
    <tr><td class="paramname">net</td><td>Pre-trained loaded ONNX neural network for object detection.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A list of Detection objects.</dd></dl>
<dl class="section note"><dt>Note</dt><dd>The format [batchSize, 84, 8400] is interpreted as: 1 - batchSize (should always be 1 since the inference is made on singular frames) 84 - 0, 1, 2, 3 are the usual YOLO bounding box [centerX, centerY, width, height], the rest are the probabilities for each class 8400 - number of possible detected objects</dd>
<dd>
The classes are hard-coded; they can be replaced to work with a specific ONNX model.</dd></dl>
<p>Example usage: </p><div class="fragment"><div class="line">nn = loadONNX()</div>
<div class="line">input_image = cv2.imread(<span class="stringliteral">&quot;path/to/image.jpeg&quot;</span>)</div>
<div class="line">detections = inference(input_image, nn)</div>
</div><!-- fragment --><dl class="section see"><dt>See also</dt><dd>padAndResize </dd>
<dd>
loadONNX </dd>
<dd>
<a href="https://github.com/ultralytics/ultralytics/issues/2670">Github issue</a> about YOLOv8 output </dd></dl>

</div>
</div>
<a id="ac996652bfe7aa7af9b5d6a679aaaaaa7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac996652bfe7aa7af9b5d6a679aaaaaa7">&#9670;&nbsp;</a></span>loadONNX()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> cv.dnn.Net vision_planner.detection.loadONNX </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Load a neural network from an ONNX file. </p>
<p>This function loads a neural network model from an ONNX trained model file and configures it for inference. If CUDA-enabled GPUs are available, and CUDA runtime is built on the system, the function sets the backend and target to CUDA for faster inference. Otherwise, it defaults to CPU processing.</p>
<dl class="section return"><dt>Returns</dt><dd>A cv2.dnn.Net object representing the loaded neural network.</dd></dl>
<dl class="section note"><dt>Note</dt><dd>This is a hard-coded setup, hence the 'yolov8.onnx' model is expected in the predefined folder</dd></dl>
<p>Example usage: </p><div class="fragment"><div class="line">nn = loadONNX()</div>
<div class="line">nn.setInput(blob)</div>
<div class="line">nn.forward(results, nn.getUnconnectedLayersNames())</div>
</div><!-- fragment --><dl class="section see"><dt>See also</dt><dd>cv2.dnn.readNetFromONNX </dd>
<dd>
cv2.cuda.getCudaEnabledDeviceCount </dd></dl>

</div>
</div>
<a id="a52e9c13985ac4d984a08e144c64eefe5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a52e9c13985ac4d984a08e144c64eefe5">&#9670;&nbsp;</a></span>padAndResize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Image vision_planner.detection.padAndResize </td>
          <td>(</td>
          <td class="paramtype">cv.Mat&#160;</td>
          <td class="paramname"><em>frame</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Resize and pad an input image to a square shape for compatibility with YOLOv8 network input. </p>
<p>This function takes an input image and resizes it to a square shape while maintaining the original aspect ratio. If the input image is different from a square shape, the aspect ratio is maintained and the rest of the image filled with zeroes (black pixels). Whichever the case, the image gets scaled to a specific square resolution.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">frame</td><td>Input cv2 image to be resized and padded.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Processed image.</dd></dl>
<dl class="section note"><dt>Note</dt><dd>The scaled image size is hard-coded to cv2.Size(640, 640), needs to be changed if the network input expects another size</dd></dl>
<p>Example usage: </p><div class="fragment"><div class="line">input_image = cv2.imread(<span class="stringliteral">&quot;path/to/image.jpeg&quot;</span>)</div>
<div class="line">input_image = padAndResize(input_image)</div>
</div><!-- fragment --> 
</div>
</div>
<a id="a7d7cdbaeafdf5527964d9d636776db96"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7d7cdbaeafdf5527964d9d636776db96">&#9670;&nbsp;</a></span>showBBox()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None vision_planner.detection.showBBox </td>
          <td>(</td>
          <td class="paramtype">cv.Mat&#160;</td>
          <td class="paramname"><em>frame</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">list&#160;</td>
          <td class="paramname"><em>detections</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Show on screen the image on which inference has been performed with the detections. </p>
<p>The function takes as input the image used for detection and the subsequent detections that the model has provided. Following this, bounding boxes are drawn on the image with the respective class names and confidence scores and shown on screen for any necessary assesment.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">frame</td><td>Input cv2 image on which inference has been performed. </td></tr>
    <tr><td class="paramname">detections</td><td>list of Detection objects with data for all the detected blocks. <br  />
</td></tr>
  </table>
  </dd>
</dl>
<p>Example usage: </p><div class="fragment"><div class="line">nn = loadONNX()</div>
<div class="line">input_image = cv2.imread(<span class="stringliteral">&quot;path/to/image.jpeg&quot;</span>)</div>
<div class="line">detections = inference(input_image, nn)</div>
<div class="line">showBBox(input_image, detections)</div>
</div><!-- fragment --><dl class="section see"><dt>See also</dt><dd>padAndResize </dd>
<dd>
loadONNX </dd>
<dd>
inference </dd></dl>

</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a8fac2cd10d176dabed674d0884371b0a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8fac2cd10d176dabed674d0884371b0a">&#9670;&nbsp;</a></span>classNames</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">list vision_planner.detection.classNames</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;=  [</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;    <span class="stringliteral">&quot;X1-Y1-Z2&quot;</span>,</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;    <span class="stringliteral">&quot;X1-Y2-Z1&quot;</span>,</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;    <span class="stringliteral">&quot;X1-Y2-Z2&quot;</span>,</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;    <span class="stringliteral">&quot;X1-Y2-Z2-CHAMFER&quot;</span>,</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;    <span class="stringliteral">&quot;X1-Y2-Z2-TWINFILLET&quot;</span>,</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;    <span class="stringliteral">&quot;X1-Y3-Z2&quot;</span>,</div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;    <span class="stringliteral">&quot;X1-Y3-Z2-FILLET&quot;</span>,</div>
<div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;    <span class="stringliteral">&quot;X1-Y4-Z1&quot;</span>,</div>
<div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;    <span class="stringliteral">&quot;X1-Y4-Z2&quot;</span>,</div>
<div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;    <span class="stringliteral">&quot;X2-Y2-Z2&quot;</span>,</div>
<div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;    <span class="stringliteral">&quot;X2-Y2-Z2-FILLET&quot;</span></div>
<div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;]</div>
</div><!-- fragment -->
<p>Classes of possibly detected objects. </p>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
